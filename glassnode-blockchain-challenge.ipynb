{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n2 - Unsupervised Learning: Clustering\nIn this part your task is to query Ethereum transactions and use unsupervised learning in order to cluster addresses. \nUsing all transactions from the last 1-2 weeks should be enough to tackle this task.\n\nThe task is deliberately held general. Here are a couple of things to consider:\n\n1. Which features do you engineer in order to feed into the clustering algorithm?\n2. Can you tell which features had the most influence on your clustering results? Do you see interesting patterns, e.g. anything in particular that is common for addresses in a particular cluster?\n3. How do you chose the inital number of clusters?\n4. Is it possible to evaluate the results of your clustering? If so, how?\n5. How would you tackle this task if you had to run feature engineering and clustering on a year of transaction data, \ne.g. the data would be too large to fit into the memory of your computer and computation of features would be computationally very expensive?\n6. What are more advanced things you can think of on how to tackle this task, but that are out of this challenge's scope?\n\"\"\"\n\nfrom google.cloud import bigquery\nimport pandas as pd\nimport numpy as np\n\nBLOCK_TIMESTAMP_FROM = \"'2019-04-20 00:00:00'\" \n#TODO: replace BLOCK_TIMESTAMP_TO with the current date\nBLOCK_TIMESTAMP_TO = \"'2019-04-27 00:00:00'\" \n\nclient = bigquery.Client()\n\nsql = \"\"\"\nSELECT *\n    FROM `bigquery-public-data.ethereum_blockchain.transactions` as t\n    WHERE t.block_timestamp >= {from_block_ts}\n    AND t.block_timestamp <= {to_block_ts}\n\"\"\".format(from_block_ts=BLOCK_TIMESTAMP_FROM, to_block_ts=BLOCK_TIMESTAMP_TO)\n\ndf_token_tran = client.query(sql).to_dataframe()\ndf_token_tran.head()\n\n# https://scikit-learn.org/stable/modules/clustering.html\n# https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n1 - Blockchain Data Insights\nHere are some questions we would like you to solve you in this task:\n\nWhich ERC20 tokens had the most activity (in terms of number of transactions) within the last week?\nWhat is the total transaction volume of each of those top tokens within that period?\nHow does the daily number of transactions of those top 5 tokens look like for the last month?\nThink about how to best represent (visualise) the results obtained from the above questions.\n\"\"\"\n\nfrom google.cloud import bigquery\nimport pandas as pd\nimport numpy as np\n\n\n# TODO: replace BLOCK_TIMESTAMP with the 7 days before the current date\n# the following instructions cause error\n# Forbidden: 403 GET https://dp.kaggle.net/bigquery/v2/projects/kaggle-161607/jobs/a0c4ed59-067c-4d34-93cd-33d79e3d47ae?location=US: Forbidden: [bigquery-public-data.ethereum_blockchain]\ndate_N_days_ago = datetime.now() - timedelta(days=7)\ndate_N_days_ago = date_N_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\ndate_N_days_ago = \"'{}'\".format(date_N_days_ago)\nBLOCK_TIMESTAMP = date_N_days_ago\n\nBLOCK_TIMESTAMP_FROM = \"'2019-04-20 00:00:00'\" \n#TODO: replace BLOCK_TIMESTAMP_TO with the current date\nBLOCK_TIMESTAMP_TO = \"'2019-04-27 00:00:00'\" \n\nclient = bigquery.Client()\n\n# SUM(CAST(tx.value AS float64)/POWER(10,18)) as amount_sent,\nsql = \"\"\"\nSELECT tx.token_address, COUNT(tx.transaction_hash) as nunm_transactions,\n    SUM(CAST(tx.value AS float64)) as total_volume_transacction,\n    MIN(block_timestamp) as start_block_timestamp,\n    {to_block_ts} as current_timestamp\n    FROM `bigquery-public-data.ethereum_blockchain.token_transfers` as tx\n    WHERE tx.block_timestamp >= {from_block_ts}\n    AND tx.block_timestamp <= {to_block_ts}\n    GROUP BY 1\n    ORDER BY nunm_transactions DESC\n    LIMIT 5\n\"\"\".format(from_block_ts=BLOCK_TIMESTAMP_FROM, to_block_ts=BLOCK_TIMESTAMP_TO)\n\ndf_token_tran = client.query(sql).to_dataframe()\n\n# 1. Which ERC20 tokens had the most activity (in terms of number of transactions) within the last week?\nprint('The ERC20 tokens had the most activity (in terms of number of transactions) within the last week')\nprint(df_token_tran.iloc[0]['token_address'])\nprint(df_token_tran.iloc[0]['nunm_transactions'])\n\n# 2. What is the total transaction volume of each of those top tokens within that period?\nprint('The total transaction volume of each of those top tokens within that period')\nprint(df_token_tran['total_volume_transacction'])\n\n# 3. How does the daily number of transactions of those top 5 tokens look like for the last month?\ndef get_daily_num_transaction(BLOCK_TIMESTAMP_FROM, BLOCK_TIMESTAMP_TO, token_address):\n    sql = \"\"\"\n    SELECT *\n    FROM `bigquery-public-data.ethereum_blockchain.token_transfers` as tx\n    WHERE tx.block_timestamp >= {from_block_ts}\n    AND tx.block_timestamp <= {to_block_ts}\n    AND tx.token_address = '{token_address}'\n    ORDER BY block_timestamp DESC\n    \"\"\".format(from_block_ts=BLOCK_TIMESTAMP_FROM, to_block_ts=BLOCK_TIMESTAMP_TO, token_address=token_address)\n\n    df_token_tran = client.query(sql).to_dataframe()\n    df_token_tran_new = df_token_tran[['block_timestamp']]\n    df_token_tran_new['block_timestamp'] = pd.to_datetime(df_token_tran['block_timestamp'])\n    \n    df_token_tran_new['num_transaction'] = 1\n    df_token_tran_new.index = df_token_tran_new['block_timestamp'] \n\n    #https://chrisalbon.com/python/data_wrangling/pandas_group_data_by_time/\n    #df_token_tran_new.resample('D').sum()\n    df_token_tran_new = df_token_tran_new.resample('D').sum()\n    return df_token_tran_new\n\n#TODO: replace BLOCK_TIMESTAMP_TO with the current date and BLOCK_TIMESTAMP_FROM with one month before\nBLOCK_TIMESTAMP_FROM = \"'2019-03-27 00:00:00'\" \nBLOCK_TIMESTAMP_TO = \"'2019-04-27 00:00:00'\" \n\n#token_address = df_token_tran.iloc[0]['token_address']\n#df_token_tran_daily = get_daily_num_transaction(BLOCK_TIMESTAMP_FROM, BLOCK_TIMESTAMP_TO, token_address)\n#print('number of daily transactions')\n#df_token_tran_daily\n\nfor token_address in df_token_tran['token_address'].tolist():\n    df_token_tran_daily = get_daily_num_transaction(BLOCK_TIMESTAMP_FROM, BLOCK_TIMESTAMP_TO, token_address)\n    print('token address: {}'.format(token_address))\n    print('number of daily transactions')\n    print(df_token_tran_daily)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nfrom google.cloud import bigquery\nimport pandas as pd\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}